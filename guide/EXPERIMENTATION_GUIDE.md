# üß™ Gu√≠a Completa de Experimentaci√≥n con Hiperpar√°metros

## üìã √çndice
1. [Scripts Disponibles](#scripts-disponibles)
2. [Estrategias de Experimentaci√≥n](#estrategias-de-experimentaci√≥n)
3. [Hiperpar√°metros Clave](#hiperpar√°metros-clave)
4. [C√≥mo Ejecutar Experimentos](#c√≥mo-ejecutar-experimentos)
5. [An√°lisis de Resultados](#an√°lisis-de-resultados)
6. [Mejores Pr√°cticas](#mejores-pr√°cticas)

---

## üõ†Ô∏è Scripts Disponibles

### 1. `hyperparameter_experiments.py` - Experimentaci√≥n Manual
Script interactivo para probar diferentes configuraciones de hiperpar√°metros.

**Uso:**
```bash
python hyperparameter_experiments.py
```

**Estrategias incluidas:**
- Comparar Learning Rates
- Comparar Batch Sizes
- Comparar N√∫mero de √âpocas
- Comparar Weight Decay (regularizaci√≥n)
- Comparar Modelos Diferentes
- Grid Search (combinaciones)
- Mejores Pr√°cticas (3 configuraciones probadas)

### 2. `optuna_optimization.py` - Optimizaci√≥n Autom√°tica
B√∫squeda autom√°tica de hiperpar√°metros usando Optuna.

**Uso:**
```bash
# 10 trials (por defecto)
python optuna_optimization.py

# 20 trials (m√°s exhaustivo)
python optuna_optimization.py --n-trials 20
```

**Qu√© optimiza:**
- Learning rate (1e-5 a 1e-4)
- Batch size (4, 8, 16)
- N√∫mero de √©pocas (2 a 5)
- Weight decay (0.001 a 0.1)

### 3. `analyze_results.py` - An√°lisis Autom√°tico
Genera reportes y gr√°ficos comparativos de tus experimentos.

**Uso:**
```bash
python analyze_results.py
```

**Genera:**
- Estad√≠sticas de todas las m√©tricas
- Top 5 mejores configuraciones
- Gr√°ficos de comparaci√≥n
- Matriz de correlaciones
- Reportes en texto

---

## üéØ Hiperpar√°metros Clave

### 1. Learning Rate (tasa de aprendizaje)
**¬øQu√© hace?** Controla qu√© tan grande es el paso en cada actualizaci√≥n del modelo.

**Valores t√≠picos:**
- `1e-5` - Muy conservador, entrenamiento lento pero estable
- `2e-5` - **Valor por defecto recomendado** para BERT/DistilBERT
- `3e-5` - M√°s agresivo, converge m√°s r√°pido
- `5e-5` - Muy agresivo, puede ser inestable

**C√≥mo afecta:**
- ‚¨áÔ∏è Muy bajo: entrenamiento muy lento, puede quedar atascado
- ‚¨ÜÔ∏è Muy alto: entrenamiento inestable, puede diverger

**Experimento sugerido:**
```bash
python hyperparameter_experiments.py
# Selecciona opci√≥n 1: Comparar Learning Rates
```

---

### 2. Batch Size (tama√±o de lote)
**¬øQu√© hace?** N√∫mero de ejemplos procesados antes de actualizar los pesos.

**Valores t√≠picos:**
- `4` - Poco uso de memoria, entrenamiento m√°s ruidoso
- `8` - **Valor por defecto recomendado**, buen balance
- `16` - M√°s r√°pido, requiere m√°s memoria
- `32` - Muy r√°pido, requiere GPU con mucha memoria

**C√≥mo afecta:**
- ‚¨áÔ∏è M√°s peque√±o: menos memoria, m√°s ruido (puede ayudar a generalizar), m√°s lento
- ‚¨ÜÔ∏è M√°s grande: m√°s memoria, m√°s estable, m√°s r√°pido

**Experimento sugerido:**
```bash
python hyperparameter_experiments.py
# Selecciona opci√≥n 2: Comparar Batch Sizes
```

---

### 3. N√∫mero de √âpocas
**¬øQu√© hace?** Cu√°ntas veces el modelo ve todo el dataset.

**Valores t√≠picos:**
- `2` - Entrenamiento r√°pido, puede no converger
- `3` - **Valor por defecto recomendado**
- `5` - M√°s entrenamiento, riesgo de overfitting
- `10+` - Solo para datasets muy grandes

**C√≥mo afecta:**
- ‚¨áÔ∏è Muy pocas: modelo no aprende suficiente (underfitting)
- ‚¨ÜÔ∏è Muchas: modelo memoriza datos (overfitting)

**Se√±ales de overfitting:**
- Training accuracy sube, validation accuracy baja
- Loss de validaci√≥n empieza a subir

**Experimento sugerido:**
```bash
python hyperparameter_experiments.py
# Selecciona opci√≥n 3: Comparar N√∫mero de √âpocas
```

---

### 4. Weight Decay (regularizaci√≥n L2)
**¬øQu√© hace?** Penaliza pesos grandes para prevenir overfitting.

**Valores t√≠picos:**
- `0.001` - Poca regularizaci√≥n
- `0.01` - **Valor por defecto recomendado**
- `0.1` - Mucha regularizaci√≥n

**C√≥mo afecta:**
- ‚¨áÔ∏è Muy bajo: modelo puede overfittear
- ‚¨ÜÔ∏è Muy alto: modelo underfittea, no aprende bien

**Experimento sugerido:**
```bash
python hyperparameter_experiments.py
# Selecciona opci√≥n 4: Comparar Weight Decay
```

---

### 5. Modelo Base
**¬øQu√© hace?** Arquitectura pre-entrenada a usar.

**Opciones:**
- `distilbert-base-uncased` - **M√°s r√°pido**, 66M par√°metros, ~250MB
- `bert-base-uncased` - M√°s preciso, 110M par√°metros, ~420MB
- `roberta-base` - Variante de BERT, 125M par√°metros
- `albert-base-v2` - M√°s ligero, 12M par√°metros

**Trade-offs:**
- ‚ö° Velocidad vs üéØ Precisi√≥n
- üíæ Memoria vs üìä Accuracy

**Experimento sugerido:**
```bash
python hyperparameter_experiments.py
# Selecciona opci√≥n 5: Comparar Modelos Diferentes
```

---

## üöÄ C√≥mo Ejecutar Experimentos

### Flujo de Trabajo Recomendado

#### **Paso 1: Baseline**
Entrena un modelo con configuraci√≥n por defecto para tener una referencia.

```bash
python -m src.train --config config_rapido.json --run-name "baseline"
```

#### **Paso 2: Exploraci√≥n R√°pida**
Prueba diferentes learning rates (el par√°metro m√°s importante).

```bash
python hyperparameter_experiments.py
# Selecciona opci√≥n 1
```

#### **Paso 3: Ajuste Fino**
Prueba combinaciones con grid search.

```bash
python hyperparameter_experiments.py
# Selecciona opci√≥n 6
```

#### **Paso 4: Optimizaci√≥n Autom√°tica** (Opcional)
Deja que Optuna encuentre la mejor configuraci√≥n.

```bash
python optuna_optimization.py --n-trials 15
```

#### **Paso 5: An√°lisis**
Compara todos los resultados.

```bash
python analyze_results.py
```

#### **Paso 6: UI de MLflow**
Visualiza los experimentos interactivamente.

```bash
mlflow ui
# Abre http://localhost:5000
```

---

## üìä An√°lisis de Resultados

### En la UI de MLflow

1. **Ver todos los runs:**
   - Abre http://localhost:5000
   - Selecciona el experimento
   - Ordena por `test_accuracy` (click en la columna)

2. **Comparar runs:**
   - Selecciona 2+ runs (checkbox)
   - Click en "Compare"
   - Ve diferencias en par√°metros y m√©tricas

3. **Gr√°ficos:**
   - En "Charts", crea gr√°ficos personalizados
   - Eje X: learning_rate, Eje Y: test_accuracy
   - Agrupa por batch_size

### Con analyze_results.py

```bash
python analyze_results.py
```

Esto genera:
- `./analysis/<experimento>/accuracy_vs_lr.png` - Accuracy vs Learning Rate
- `./analysis/<experimento>/accuracy_vs_batch.png` - Accuracy vs Batch Size
- `./analysis/<experimento>/correlation_heatmap.png` - Correlaciones
- `./analysis/<experimento>/metrics_comparison.png` - Comparaci√≥n de m√©tricas
- `./analysis/<experimento>/report.txt` - Reporte en texto

---

## üéì Mejores Pr√°cticas

### 1. **Empieza simple, itera despu√©s**
```bash
# Primero: baseline
python -m src.train --config config_rapido.json --run-name "baseline"

# Luego: variar UN par√°metro a la vez
# Learning rate
# Batch size
# Etc.
```

### 2. **Usa nombres descriptivos**
```bash
python -m src.train \
  --run-name "distilbert-lr3e5-bs16-ep3" \
  --experiment-name "production-tuning"
```

### 3. **Documenta tus hallazgos**
Crea un `EXPERIMENT_LOG.md`:
```markdown
## 2025-10-24: Learning Rate Comparison

**Objetivo:** Encontrar mejor learning rate

**Resultados:**
- lr=2e-5: accuracy=0.85
- lr=3e-5: accuracy=0.87 ‚úÖ MEJOR
- lr=5e-5: accuracy=0.83 (inestable)

**Conclusi√≥n:** Usar 3e-5 para futuros experimentos
```

### 4. **Prioriza par√°metros por impacto**

**Alto impacto** (probar primero):
1. Learning rate
2. N√∫mero de √©pocas
3. Modelo base

**Medio impacto**:
4. Batch size
5. Weight decay

**Bajo impacto** (ajuste fino):
6. Warmup steps
7. Learning rate scheduler

### 5. **Valida con datos de test separados**
Aseg√∫rate de que `config.json` tenga:
```json
"data": {
  "train_size": 4000,
  "eval_size": 1000,  // Para early stopping
  "test_size": 500     // Para evaluaci√≥n final
}
```

### 6. **Detecta overfitting**
En MLflow, compara:
- `eval_accuracy` vs `test_accuracy`
- Si `eval_accuracy` >> `test_accuracy`: overfitting

Soluciones:
- Aumentar `weight_decay`
- Reducir √©pocas
- Usar m√°s datos de entrenamiento

---

## üîç Interpretaci√≥n de M√©tricas

### Accuracy (exactitud)
- **Qu√© es:** % de predicciones correctas
- **Objetivo:** Maximizar
- **Bueno:** >0.85 para sentiment analysis en IMDB
- **Excelente:** >0.90

### F1-Score
- **Qu√© es:** Balance entre precision y recall
- **Objetivo:** Maximizar
- **Mejor que accuracy cuando:** clases desbalanceadas

### Loss (p√©rdida)
- **Qu√© es:** Qu√© tan "equivocado" est√° el modelo
- **Objetivo:** Minimizar
- **T√≠pico:** 0.2-0.5 para modelos buenos

### Precision vs Recall
- **Precision:** De las predicciones positivas, ¬øcu√°ntas son correctas?
- **Recall:** De los casos positivos reales, ¬øcu√°ntos detectamos?

---

## üéØ Ejemplos de Escenarios

### Escenario 1: "Mi modelo no mejora"
**S√≠ntomas:**
- Accuracy se queda en ~0.50 (random)
- Loss no baja

**Posibles causas:**
- Learning rate muy bajo ‚Üí Prueba 3e-5 o 5e-5
- Muy pocas √©pocas ‚Üí Aumenta a 5
- Modelo muy simple ‚Üí Prueba `bert-base` en vez de `distilbert`

**Soluci√≥n:**
```bash
python hyperparameter_experiments.py
# Opci√≥n 7: Mejores Pr√°cticas - configuraci√≥n "aggressive"
```

### Escenario 2: "Overfitting claro"
**S√≠ntomas:**
- Training accuracy: 0.95
- Test accuracy: 0.75

**Soluciones:**
```bash
# Aumentar regularizaci√≥n
# Modifica config.json: "weight_decay": 0.1

# Reducir √©pocas
# Modifica config.json: "num_train_epochs": 2

# Entrenar
python -m src.train --config config.json --run-name "fix-overfit"
```

### Escenario 3: "Quiero el mejor modelo posible"
**Estrategia:**
```bash
# 1. Optimizaci√≥n autom√°tica
python optuna_optimization.py --n-trials 20

# 2. Tomar mejores par√°metros de Optuna
# (se guardan en ./experiments/optuna/best_config.json)

# 3. Entrenar con dataset completo
python -m src.train \
  --config ./experiments/optuna/best_config.json \
  --run-name "final-model-full-data"

# 4. Evaluar
mlflow ui
```

---

## üìö Recursos Adicionales

- **Hugging Face Training Tips:** https://huggingface.co/docs/transformers/training
- **Learning Rate Finder:** https://arxiv.org/abs/1506.01186
- **Optuna Documentation:** https://optuna.readthedocs.io/

---

## ‚úÖ Checklist de Experimentaci√≥n

Antes de decidir que tu modelo est√° "listo":

- [ ] Entrenaste un baseline
- [ ] Probaste al menos 3 learning rates diferentes
- [ ] Verificaste que no hay overfitting
- [ ] Comparaste al menos 2 modelos base
- [ ] Evaluaste en un test set separado
- [ ] Documentaste tus resultados
- [ ] Guardaste el mejor modelo en MLflow Model Registry

---

**√öltima actualizaci√≥n:** 24 de octubre de 2025  
**Autor:** Configuraci√≥n autom√°tica MLflow  
**Versi√≥n:** 1.0
